{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c4be40-c427-412b-8f79-a8e9ac70037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0pc5f7   10  100  1000  10000  10min   11  110  1130  11am  ...   xr  \\\n",
      "0     0.0  0.0  0.0   0.0    0.0    0.0  0.0  0.0   0.0   0.0  ...  0.0   \n",
      "1     0.0  0.0  0.0   0.0    0.0    0.0  0.0  0.0   0.0   0.0  ...  0.0   \n",
      "2     0.0  0.0  0.0   0.0    0.0    0.0  0.0  0.0   0.0   0.0  ...  0.0   \n",
      "3     0.0  0.0  0.0   0.0    0.0    0.0  0.0  0.0   0.0   0.0  ...  0.0   \n",
      "4     0.0  0.0  0.0   0.0    0.0    0.0  0.0  0.0   0.0   0.0  ...  0.0   \n",
      "\n",
      "   yakuza  yakuzas  yendo  york  your  youtube  yuda  yulay  zonas  \n",
      "0     0.0      0.0    0.0   0.0   0.0      0.0   0.0    0.0    0.0  \n",
      "1     0.0      0.0    0.0   0.0   0.0      0.0   0.0    0.0    0.0  \n",
      "2     0.0      0.0    0.0   0.0   0.0      0.0   0.0    0.0    0.0  \n",
      "3     0.0      0.0    0.0   0.0   0.0      0.0   0.0    0.0    0.0  \n",
      "4     0.0      0.0    0.0   0.0   0.0      0.0   0.0    0.0    0.0  \n",
      "\n",
      "[5 rows x 4527 columns]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 26 15:09:20 2024\n",
    "\n",
    "@author: nanor\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Eliminar caracteres especiales y signos de puntuación\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "def convert_to_lowercase(text):\n",
    "    # Convertir texto a minúsculas\n",
    "    lowercase_text = text.lower()\n",
    "    return lowercase_text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenizar el texto en palabras individuales\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Obtener lista de stopwords en español\n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "    # Eliminar stopwords del texto tokenizado\n",
    "    filtered_tokens = [word for word in tokens if word not in spanish_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Preprocesar texto\n",
    "    clean_text = remove_special_characters(text)\n",
    "    clean_text = convert_to_lowercase(clean_text)\n",
    "    tokens = tokenize_text(clean_text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Leer el texto desde un archivo .txt\n",
    "file_path = 'reddit_posts.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Dividir el archivo en posts individuales\n",
    "posts = text.split('\\n\\n')\n",
    "\n",
    "# Aplicar preprocesamiento a cada post\n",
    "preprocessed_posts = [preprocess_text(post) for post in posts]\n",
    "\n",
    "# Guardar los posts preprocesados en un nuevo archivo\n",
    "preprocessed_file_path = 'preprocessed_reddit_posts.txt'\n",
    "with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
    "    for post in preprocessed_posts:\n",
    "        file.write(\"%s\\n\\n\" % post)\n",
    "\n",
    "# Vectorizar los posts preprocesados utilizando TF-IDF\n",
    "#vectorizer = TfidfVectorizer(max_features=1000)  # Puedes ajustar el número de características\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Puedes ajustar el número de características\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_posts)\n",
    "\n",
    "# Convertir la matriz TF-IDF a un DataFrame de pandas para su análisis\n",
    "# Usa get_feature_names en lugar de get_feature_names_out\n",
    "#tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Guardar la matriz TF-IDF en un archivo CSV para su posterior análisis\n",
    "tfidf_df.to_csv('conteo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
